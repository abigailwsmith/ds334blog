[
  {
    "objectID": "posts/Blog-Post-1/index.html",
    "href": "posts/Blog-Post-1/index.html",
    "title": "Blog Post 1",
    "section": "",
    "text": "Introduction\nGroundhog day is an amusing American tradition celebrated on February 2nd, in which a groundhog either sees its shadow, meaning winter will extend 6 more weeks, or it doesn’t, meaning Spring will arrive early. The tradition is said to have originated from German-Americans and is especially popular in Pennsylvania, as there is a high German-American population there.\nI think that it will be fun and interesting to look at data gathered from these celebrations, which is why for my first blog post I chose to work with the groundhog dataset from the TidyTuesday Repo on GitHub. The dataset is made up of two individual dataframes, entitled “groundhogs” and “predictions”. The “groundhogs” data provides information about the groundhogs’ names, states, and various other variables. The “predictions” dataset provides information about the predictions made by the groundhogs, measured in the shadow variable, these predictions have been gathered between 1887 and 2023. Additionally, I used the state_df which provides geographic information on different US states, such as their latitude and longitude. This will be useful to make a map.\nI will need to join the two groundhog dataframes into one big datafrane using their id numbers. In this full dataset I am interested in looking at the shadow and region variables.\n\n\nQuestion of interest\nIs there a clear trend in predictions based on region? To be specific, are shadow sightings more common in Northern areas where it is more common to have longer winters in general?\n\n\nData set links\nGroundhogs: https://github.com/rfordatascience/tidytuesday/blob/66f1fa48a2c09131e5500a7bf12201c6581f28fa/data/2024/2024-01-30/groundhogs.csv &\nPredictions: https://github.com/rfordatascience/tidytuesday/blob/66f1fa48a2c09131e5500a7bf12201c6581f28fa/data/2024/2024-01-30/predictions.csv).\nState_df: https://cran.r-project.org/web/packages/maps/maps.pdf\n\n\nRead in the data and packages\n\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(maps)\nlibrary(here)\nstate_df &lt;- ggplot2::map_data(\"state\")\ngroundhogs &lt;- read_csv(here(\"data/groundhogs.csv\"))\npredictions &lt;- read_csv(here(\"data/predictions.csv\"))\n\n\n\nTidying, wrangling, and joining the data\nFirst I had to join the “groundhogs” and “predictions” datasets to combine the two into one dataset which will have information about the groundhogs as well as their predictions.\n\ngroundhogs_pred &lt;- left_join(groundhogs, predictions, by = \"id\")\ngroundhogs_pred\n\n# A tibble: 1,462 × 20\n      id slug     shortname name  city  region country latitude longitude source\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; \n 1     1 punxsut… Phil      Punx… Punx… Penns… USA         40.9     -79.0 https…\n 2     1 punxsut… Phil      Punx… Punx… Penns… USA         40.9     -79.0 https…\n 3     1 punxsut… Phil      Punx… Punx… Penns… USA         40.9     -79.0 https…\n 4     1 punxsut… Phil      Punx… Punx… Penns… USA         40.9     -79.0 https…\n 5     1 punxsut… Phil      Punx… Punx… Penns… USA         40.9     -79.0 https…\n 6     1 punxsut… Phil      Punx… Punx… Penns… USA         40.9     -79.0 https…\n 7     1 punxsut… Phil      Punx… Punx… Penns… USA         40.9     -79.0 https…\n 8     1 punxsut… Phil      Punx… Punx… Penns… USA         40.9     -79.0 https…\n 9     1 punxsut… Phil      Punx… Punx… Penns… USA         40.9     -79.0 https…\n10     1 punxsut… Phil      Punx… Punx… Penns… USA         40.9     -79.0 https…\n# ℹ 1,452 more rows\n# ℹ 10 more variables: current_prediction &lt;chr&gt;, is_groundhog &lt;lgl&gt;,\n#   type &lt;chr&gt;, active &lt;lgl&gt;, description &lt;chr&gt;, image &lt;chr&gt;,\n#   predictions_count &lt;dbl&gt;, year &lt;dbl&gt;, shadow &lt;lgl&gt;, details &lt;chr&gt;\n\n\nThe “groundhogs_pred” dataset has some groundhogs in Canada in addition to the US. For the purpose of my investigation, I only want to look at groundhogs in the US so I filtered the dataset to only include those. Some of the earlier groundhog days in the dataset do not have anything recorded for the shadow variable so I filtered out those NA cases. Lastly, I changed the names of the states in the region variable to lowercase so that they would match the names in the state_df.\n\ngroundhogs_pred &lt;- groundhogs_pred |&gt; filter(country == \"USA\") |&gt; filter(!is.na(shadow)) |&gt; mutate(region = tolower(region))\n\nNext, I found the number of times groundhogs in each state saw their shadow by grouping by region and using summarise to create the n_shadows variable.\n\ngroundhogs_sum &lt;- groundhogs_pred |&gt; group_by(region) |&gt; summarise(n_shadows = sum(shadow)) \n\nThen, I joined the summarised groundhogs data with the state_df so that I would be able to create a map of the groundhog shadow sightings in the US.\n\nsum_state_groundhog &lt;- right_join(groundhogs_sum, state_df, by = \"region\")\n\n\n\nMap of the Number of Groundhog Sightings in the US between 1887 and 2023\n\nggplot(data = sum_state_groundhog, aes(x = long, y = lat, group = group)) +\n  geom_polygon(colour = \"black\", aes(fill = n_shadows)) +\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45) +\n  theme_void() +\n  scale_fill_viridis_c(option = \"plasma\") + labs(title = \"Number of Groundhog Sightings in the US between 1887 and 2023\", subtitle = \"*states in grey had no groundhog shadow sightings\") +  theme(plot.title = element_text(hjust = 0.5, size = 15), plot.subtitle = element_text(hjust = 0.5, size = 7))\n\n\n\n\nUsing the maps package as well as the sum_state_groundhog dataset I created, I was able to make the above map. As its title states, the map depicts “The Number of Groundhog Sightings in the US between 1887 and 2023”. One of the first things to stand out in the map, unsurprisingly, is Pennsylvania which is highlighted in yellow as the state with the highest number of shadow sightings. This makes sense because Punxsutawney, Pennsylvania is where the main Groundhog Day ceremony is held, so thus, there would be a high number of recordings of Groundhog Day results, leading to a higher number of shadow sightings by default. In addition to Punxsutawney, there are several other small towns in Pennsylvania that participate in the tradition, so there would be a bigger number of recordings and thus shadow sightings.\nInterestingly, it does seem that overall states in the north and northeast particular seem to have a higher number of shadow sightings which relates back to my question of interest. In colder states, groundhogs see their shadows more often, thus predicting a longer winter more frequently.\nHowever, not all northern states have this. In fact many of the states are grey which means that they have no shadow sightings recorded in the dataset. This is interesting because this either means that the states had no Groundhog Day recordings at all or they simply didn’t have shadow sightings.\n\ngroundhogs_lolli &lt;- groundhogs_sum |&gt; mutate(region = fct_reorder(region, n_shadows))\n\nggplot(data = groundhogs_lolli, aes(x = region, y = n_shadows)) + labs(title = \"Number of Groundhog Sightings in the US between 1887 and 2023\") + geom_segment(aes(xend = region, y = 0, yend = n_shadows)) +\n  geom_point() + coord_flip() + theme(plot.title = element_text(hjust = 0.5, size = 15), plot.subtitle = element_text(hjust = 0.5, size = 7)) + theme_minimal()\n\n\n\n\nAll of this can also be seen in the above lollipop plot, which is a simplified rendition of the same data. The lollipop plot highlights just how insufficient using the number of shadow sightings is for representing the trends in the data. Using only the number of shadow sightings does not give a sense of how common or uncommon a shadow sighting is. Therefore, I made a new dataset with a variable calculating the proportion of shadow sightings in the US. I then used this new dataset to make a new map and lollipop plot using the proportion instead.\n\ngroundhogs_prop &lt;- groundhogs_pred |&gt; group_by(region) |&gt; summarise(prop_shadows = mean(shadow)) \n\nprop_state_groundhog &lt;- right_join(groundhogs_prop, state_df, by = \"region\")\n\nggplot(data = prop_state_groundhog, aes(x = long, y = lat, group = group)) +\n  geom_polygon(colour = \"black\", aes(fill = prop_shadows)) +\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45) +\n  theme_void() +\n  scale_fill_viridis_c(option = \"plasma\") + labs(title = \"Proportion of Groundhog Sightings in the US between 1887 and 2023\", subtitle = \"*states in grey had no groundhog shadow sightings\") +  theme(plot.title = element_text(hjust = 0.5, size = 15), plot.subtitle = element_text(hjust = 0.5, size = 7))\n\n\n\n\nAs predicted, the proportion is a much more effective method at looking at the trends in data in both plots. The map, in particular, clearly exhibits greater variance than the previous map using only the number of shadow sightings. Interestingly, however, instead of Pennsylvania having the highest proportion of sightings, it is California. This is likely because there have been fewer Groundhog Days in California, leaving a smaller sample size to calculate the proportion from, thus making the proportion deceptively high. But as shown in the below lollipop plot as well, the proportion does give a better sense of the variance of the data, which makes it overall more effective for examining the trends in the data.\n\ngroundhogs_lolli_prop &lt;- groundhogs_prop |&gt; mutate(region = fct_reorder(region, prop_shadows))\n\nggplot(data = groundhogs_lolli_prop, aes(x = region, y = prop_shadows)) + labs(title = \"Proportion of Groundhog Sightings in the US between 1887 and 2023\") + geom_segment(aes(xend = region, y = 0, yend = prop_shadows)) +\n  geom_point() + coord_flip() + theme(plot.title = element_text(hjust = 0.5, size = 15), plot.subtitle = element_text(hjust = 0.5, size = 7)) + theme_minimal()\n\n\n\n\n\n\nConclusion and wrap-up\nOverall, I would say that my primary visualization methods did not necessarily work perfectly to visualize the data. However, the ability to use maps to graph and visualize the data makes it more interesting, and easier to get a sense of the spread of groundhog sightings in the US. I think this can be helpful in cases like this where geographic information is important, especially since with this data in particular I was interested in how the trend in shadows relates to the geographical layout of the US. If I had more time I would be interested in researching the shadow sightings over the course of time. I also think it would be interesting to look at the differences between rural and urban communities groundhog recordings. I also would have liked to be able to categorize states as either north or south to investigate the data.\n\n\nConnection to class ideas\nI used a lot of the ideas we learned in class to visualize the data I was working with. One of the main things I was able to work with was to be able to adjust the specifics of the theme of my graphs to make them easier to read and better to look at. I also used the color scale concept that we learned, using a continuous or sequential scale on both of my maps since both the prop_shadows and n_shadows variables are continuous variables. I would say that my graphs, especially the maps, are an effective way of displaying the trends of data in both the proportion and number of groundhog shadow sightings in the US. The maps are particularly effective because they visualize the data in geographical context. One drawback however is there are many missing values, which makes thr graph less complete. But I would say generally speaking the graphs are an effective means of displaying the data."
  },
  {
    "objectID": "posts/blog_post_3/index.html",
    "href": "posts/blog_post_3/index.html",
    "title": "Patterns in Global Refugee Migration",
    "section": "",
    "text": "Introduction\nThe refugee crisis has been a global issue for a long time and is a heavily debated topic. Wars and natural disasters regularly lead to the displacement of people from their home countries (origin countries). As they are forced to escape to other countries (asylum countries) seeking asylum, those new countries are faced with how to find the means to support the new influx of people which can at times strain their own economies. In many ways, however, refugees can be beneficial to asylum countries, contributing economically and socially to their structure.\nOn the tidy tuesday repository, I was able to find a very large data set called the population.csv which contains information about global refugee migration since 2010 collected by the United Nations High Commissioner for Refugees. Each row in the data set is a year with a particular combination of countries. In each row there is a country of origin variable and an asylum country variable. It is important to note that in the data set there are some countries where the asylum and origin country are the same, this is because of internally displaced people (IDPS). IDPS are people who are refugees within the same country, which is why the origin and asylum country is the same.\n\n\nQuestion of interest\nWhich countries accept the most refugees and the least? What is the overall trend in displaced peoples over time? Which are the most common countries of origin?\n\n\nData set link\npopualtion.csv: https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-08-22/readme.md\n\n\nRead in the data and packages\n\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(here)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(ggrepel)\nrefugees_df &lt;- read_csv(here(\"data/population.csv\"))\nrefugees_df\n\n# A tibble: 64,809 × 16\n    year coo_name   coo   coo_iso coa_name coa   coa_iso refugees asylum_seekers\n   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n 1  2010 Afghanist… AFG   AFG     Afghani… AFG   AFG            0              0\n 2  2010 Iran (Isl… IRN   IRN     Afghani… AFG   AFG           30             21\n 3  2010 Iraq       IRQ   IRQ     Afghani… AFG   AFG            6              0\n 4  2010 Pakistan   PAK   PAK     Afghani… AFG   AFG         6398              9\n 5  2010 Egypt      ARE   EGY     Albania  ALB   ALB            5              0\n 6  2010 China      CHI   CHN     Albania  ALB   ALB            6              0\n 7  2010 Palestini… GAZ   PSE     Albania  ALB   ALB            5              0\n 8  2010 Iraq       IRQ   IRQ     Albania  ALB   ALB            5              0\n 9  2010 Serbia an… SRB   SRB     Albania  ALB   ALB           49             20\n10  2010 Türkiye    TUR   TUR     Albania  ALB   ALB            5              0\n# ℹ 64,799 more rows\n# ℹ 7 more variables: returned_refugees &lt;dbl&gt;, idps &lt;dbl&gt;, returned_idps &lt;dbl&gt;,\n#   stateless &lt;dbl&gt;, ooc &lt;dbl&gt;, oip &lt;dbl&gt;, hst &lt;dbl&gt;\n\n\n\n\nTidying and wrangling the data for a network graph\nFirst I cleaned the data a bit, getting rid of all the unnecessary variables such as all the code ones and renaming the country of origin and country of asylum variables.\n\nrefugees_clean &lt;- refugees_df |&gt; rename(origin_country = coo_name) |&gt; rename(asylum_country = coa_name) |&gt; select(-coo, -coa, -coo_iso, -coa_iso)\n\nNext, I pivoted the dataset, creating two new variables, the type of migration the country is listed for, as well as the country’s name.\n\nrefugees_long &lt;- refugees_clean |&gt; \n  pivot_longer(c(origin_country, asylum_country),\n               names_to = \"type\",\n               values_to = \"country\") |&gt; relocate(country, type)\n\nThen I created a network data frame, renaming the origin and asylum variables as vertex 1 and 2 respectively. I then made a summarized data frame to make sure no combinations get used twice just because their order is different.\n\nnetworkdf &lt;- refugees_clean |&gt; rename(vertex1 = origin_country, vertex2 = asylum_country)\n\nnetworksum &lt;- networkdf |&gt; mutate(origincountry = if_else(vertex1 &gt; vertex2,\n                                           true = vertex2,\n                                           false = vertex1),\n                     asylumcountry = if_else(vertex1 &gt; vertex2,\n                                            true = vertex1,\n                                            false = vertex2)) |&gt;\n  dplyr::select(-vertex1, -vertex2) |&gt;\n  group_by(origincountry, asylumcountry) |&gt;\n  summarise(n_countries = n())\n\n`summarise()` has grouped output by 'origincountry'. You can override using the\n`.groups` argument.\n\n\nI then created the nodes data frame.\n\nnodes &lt;- refugees_clean |&gt;\n  pivot_longer(c(origin_country, asylum_country),\n               names_to = \"type\",\n               values_to = \"country\") |&gt;\n  distinct(country) |&gt; \n  rename(label = country) |&gt;\n  filter(label %in% c(networksum |&gt; pull(origincountry),\n                      networksum |&gt; pull(asylumcountry))) |&gt;\n  rowid_to_column(\"id\")\nnodes\n\n# A tibble: 212 × 2\n      id label                               \n   &lt;int&gt; &lt;chr&gt;                               \n 1     1 Afghanistan                         \n 2     2 Iran (Islamic Rep. of)              \n 3     3 Iraq                                \n 4     4 Pakistan                            \n 5     5 Egypt                               \n 6     6 Albania                             \n 7     7 China                               \n 8     8 Palestinian                         \n 9     9 Serbia and Kosovo: S/RES/1244 (1999)\n10    10 Türkiye                             \n# ℹ 202 more rows\n\n\nThen the edges data frame.\n\nedges &lt;- left_join(networksum, nodes, by = join_by(\"origincountry\" == label)) |&gt;\n  rename(from = \"id\") |&gt;\n  left_join(nodes, by = join_by(\"asylumcountry\" == label)) |&gt;\n  rename(to = \"id\") |&gt;\n  ungroup() |&gt;\n  dplyr::select(to, from, n_countries)\nedges\n\n# A tibble: 6,952 × 3\n      to  from n_countries\n   &lt;int&gt; &lt;int&gt;       &lt;int&gt;\n 1     1     1          13\n 2     6     1           3\n 3    12     1           1\n 4    52     1           9\n 5    53     1           6\n 6    83     1          13\n 7   122     1          13\n 8    80     1          13\n 9    54     1           5\n10    85     1          13\n# ℹ 6,942 more rows\n\n\nThen using the nodes and edges I made a table for the graph.\n\nnetworkobj &lt;- tbl_graph(nodes = nodes, edges = edges, directed = TRUE)\n\n\n\nNetwork graph\nUsing the table I just made, I created the below network graph of global refugee migration between 2010 and 2022.\n\nggraph(networkobj |&gt; mutate(centrality = centrality_authority()),\n                            layout = \"stress\") +\n  geom_edge_link(aes(width = n_countries), alpha = 0.1, show.legend = FALSE, colour = \"blue\") +\n  geom_node_point(aes(colour = label,\n                      size = centrality), show.legend = FALSE) +\n  scale_edge_width(range = c(0.5, 3)) +\n  geom_node_text(aes(label = label), repel = TRUE)  + labs(title = \"Global Refugee Migration Between 2010 and 2022\", subtitle = \"Source: United Nations High Commissioner for Refugees\")\n\nWarning: ggrepel: 124 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\nThe above network graph is honestly quite challenging to interpret as the data frame is so large, it hard to distinguish node from node and edge from edge. However, I feel that the graph being hard to read still speaks volumes, it shows just how big of an issue refugee migration is. And even though it is hard to interpret, it is still eligible what countries play bigger roles in refugee migration than others, or rather it is easier to distinguish between the smaller countries since they are on the outside, while most of the asylum nations are on the inside.\n\n\nRevised network graph\nIn an attempt to rectify the issue of such a big and confusing network graph, I tried to make it with a smaller and cleaner data set which only includes combinations where the origin and asylum country are different, meaning there are no internally displaced people.\n\nno_idps &lt;- refugees_clean |&gt; filter(origin_country != asylum_country) \n\nno_idpslong &lt;- no_idps |&gt; pivot_longer(c(origin_country, asylum_country),\n               names_to = \"type\",\n               values_to = \"country\") |&gt; relocate(country, type)\n\nThen I created a network data frame, renaming the origin and asylum variables as vertex 1 and 2 respectively. I then made a summarized data frame to make sure no combinations get used twice just because their order is different.\n\nno_idps_network &lt;- no_idps |&gt; rename(vertex1 = origin_country, vertex2 = asylum_country)\n\nsum_no_idps &lt;- no_idps_network |&gt; mutate(origincountry = if_else(vertex1 &gt; vertex2,\n                                           true = vertex2,\n                                           false = vertex1),\n                     asylumcountry = if_else(vertex1 &gt; vertex2,\n                                            true = vertex1,\n                                            false = vertex2)) |&gt;\n  dplyr::select(-vertex1, -vertex2) |&gt;\n  group_by(origincountry, asylumcountry) |&gt;\n  summarise(n_countries = n())\n\n`summarise()` has grouped output by 'origincountry'. You can override using the\n`.groups` argument.\n\n\nI then created the nodes data frame.\n\nnodes_noidps &lt;- no_idps |&gt;\n  pivot_longer(c(origin_country, asylum_country),\n               names_to = \"type\",\n               values_to = \"country\") |&gt;\n  distinct(country) |&gt; \n  rename(label = country) |&gt;\n  filter(label %in% c(sum_no_idps |&gt; pull(origincountry),\n                      sum_no_idps |&gt; pull(asylumcountry))) |&gt;\n  rowid_to_column(\"id\")\nnodes_noidps\n\n# A tibble: 212 × 2\n      id label                               \n   &lt;int&gt; &lt;chr&gt;                               \n 1     1 Iran (Islamic Rep. of)              \n 2     2 Afghanistan                         \n 3     3 Iraq                                \n 4     4 Pakistan                            \n 5     5 Egypt                               \n 6     6 Albania                             \n 7     7 China                               \n 8     8 Palestinian                         \n 9     9 Serbia and Kosovo: S/RES/1244 (1999)\n10    10 Türkiye                             \n# ℹ 202 more rows\n\n\nThen the edges data frame.\n\nedges_noidps &lt;- left_join(sum_no_idps, nodes_noidps, by = join_by(\"origincountry\" == label)) |&gt;\n  rename(from = \"id\") |&gt;\n  left_join(nodes, by = join_by(\"asylumcountry\" == label)) |&gt;\n  rename(to = \"id\") |&gt;\n  ungroup() |&gt;\n  dplyr::select(to, from, n_countries)\nedges_noidps\n\n# A tibble: 6,871 × 3\n      to  from n_countries\n   &lt;int&gt; &lt;int&gt;       &lt;int&gt;\n 1     6     2           3\n 2    12     2           1\n 3    52     2           9\n 4    53     2           6\n 5    83     2          13\n 6   122     2          13\n 7    80     2          13\n 8    54     2           5\n 9    85     2          13\n10   132     2          13\n# ℹ 6,861 more rows\n\n\nThen using the nodes and edges I made a table for the graph.\n\nobjnoidps &lt;- tbl_graph(nodes = nodes_noidps, edges = edges_noidps, directed = TRUE)\n\nUsing the table I just made, I created the below network graph of global refugee migration between 2010 and 2022 without internally displaced people.\n\nggraph(objnoidps |&gt; mutate(centrality = centrality_authority()),\n                            layout = \"stress\") +\n  geom_edge_link(aes(width = n_countries), alpha = 0.05, show.legend = FALSE, colour = \"blue\") +\n  geom_node_point(aes(colour = label,\n                      size = centrality), show.legend = FALSE) +\n  scale_edge_width(range = c(0.5, 3)) +\n  geom_node_text(aes(label = label), repel = TRUE)  + labs(title = \"Global Refugee Migration Between 2010 and 2022 Without Internally Displaced People\", subtitle = \"Source: United Nations High Commissioner for Refugees\")\n\nWarning: ggrepel: 133 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\nWhile the above graph is slightly better, it is not a big improvement, likely because this “smaller and cleaner” data set is still extremely large. But it is still slightly better. Other ways to improve it would be to sort the countries into regions instead of individual nations, however, I was unsure to do this as it would be tedious and I was not 100% sure how to classify each country.\n\n\nLineplot of total displaced people over time\nTo make a graph of the total amount of refugees displaced between 2010 and 2022, I made a new dataset with a variable total_displaced which is the sum of all the different “types” of refugees in the data set.\n\ntotaldisplaced &lt;- refugees_clean |&gt; filter(origin_country != asylum_country) |&gt; mutate(total_displaced = refugees + asylum_seekers + idps + stateless) |&gt; relocate(total_displaced) |&gt; arrange(desc(total_displaced)) |&gt; pivot_longer(c(origin_country, asylum_country),\n               names_to = \"type\",\n               values_to = \"country\") |&gt; mutate(year = as.integer(year))\n\n\nlineplot &lt;- totaldisplaced |&gt; group_by(year) |&gt; summarise(total_refugees = sum(total_displaced)) \nlineplot\n\n# A tibble: 13 × 2\n    year total_refugees\n   &lt;int&gt;          &lt;dbl&gt;\n 1  2010       29695734\n 2  2011       29553436\n 3  2012       29548494\n 4  2013       32660852\n 5  2014       39342496\n 6  2015       46042990\n 7  2016       46312026\n 8  2017       53768110\n 9  2018       55426326\n10  2019       57561098\n11  2020       58052232\n12  2021       60561604\n13  2022       78560268\n\nggplot(data = lineplot, aes(x = year, y = total_refugees)) + geom_line() + theme_minimal() + labs(title = \"Change in the amount of total refugees over time\", subtitle = \"Source: United Nations High Commissioner for Refugees\")\n\n\n\n\nAs can be seen in the above line plot, the amount of total refugees in the data set has increased over time between 2010 and 2022. This could be the result of a series of reasons, the war in Ukraine for example in 2022 could explain the sharp increase seen in the slope of the plot. Another reason why the number of refugees has increased over time is likely due to the effects of increasing climate change. Climate change has led to an abnormally high number of natural disasters in recent years, displacing even more people.\n\n\nBarplots of origin and asylum countries\nI made two separate data sets, one with only origin countries and the other with only asylum countries. I then created summarized versions of these data sets to see the number of times each country comes up as an origin and/or asylum country, to show which countries are sending and receiving the most refugees. I also arranged both data sets in order of descending total_displaced and sliced the top 100, this will just make the data sets smaller so that the graphs will not be too over crowded.\n\norigin &lt;- totaldisplaced |&gt; filter(type == \"origin_country\") |&gt; filter(country != \"Stateless\" & country != \"Unknown\") |&gt; arrange(desc(total_displaced)) |&gt; slice(1:100)\n\nsum_origin &lt;- origin |&gt;  group_by(country) |&gt; summarise(n = n())\n\nasylum &lt;- totaldisplaced |&gt; filter(type == \"asylum_country\") |&gt; filter(country != \"Stateless\" & country != \"Unknown\") |&gt; arrange(desc(total_displaced)) |&gt; slice(1:100)\n\nsum_asylum &lt;- asylum |&gt; group_by(country) |&gt; summarise(n = n())\n\n\nbarplot_origin &lt;- sum_origin |&gt; mutate(country = fct_reorder(country, n))\n\nggplot(data = barplot_origin, aes(x = country, y = n)) + geom_col() + coord_flip() + theme_minimal() + labs(y = \"Number of Times as an Origin Country\", title = \"Top 9 Most Common Origin Countries\", subtitle = \"Source: United Nations High Commissioner for Refugees\")\n\n\n\n\nThe above barplot shows that the most common origin country is the Syrian Arab Republic, which makes sense since the Syrian Civil War has been ongoing since 2011, causing a lot of refugees to flee. The other countries, such as South Sudan and Afghanistan make sense since they have all been involved in several ongoing conflicts. Interestingly while, Ukraine did make it on this top list, it is far down in the graph. this is because the war started in 2022 which is the last year of the data set meaning that the war was pretty early on when the data was collected. Additionally, Ukraine is a relatively small country so it would by default have less refugees.\n\nbarplot_asylum &lt;- sum_asylum |&gt; mutate(country = fct_reorder(country, n)) \n\nggplot(data = barplot_asylum, aes(x = country, y = n)) + geom_col() + coord_flip() + theme_minimal() + labs(y = \"Number of Times as an Asylum Country\", title = \"Top 16 Most Common Asylum Countries\", subtitle = \"Source: United Nations High Commissioner for Refugees\")\n\n\n\n\nThe above barplot of the top asylum countries is more surprising than the one of the origin countries, since some of the countries are the same. My theory of this is that often times, refugees go to nearby countries at the very least as an initial step towards safety, regardless of whether or not these countries are in conflict themselves. Turkey makes sense because of its location on the Mediterranean, making it accessible from countries in Africa and the Middle East. Poland makes a lot of sense because it neighbors Ukraine and is thus the first step of safety for many Ukrainian refugees. Germany is also an interesting country to note as they have been well known as one of the European countries that accepts the most refugees.\n\n\nConclusion and wrap-up\nIn looking through this data and examining these visualizations I learned a few things. For one thing, refugee migration is even more expansive than I thought. Also having too large a data set can be very challenging to work with. If I had more time I would have liked to sort through the data even more so that the visualizations could be easier to interpret. As I mentioned, it would have been nice to sort the countries into different regions but this could be tedious and susceptible to bias. I think that if I could have found a way to make the network graphs so that they can read from left to right with origin countries on the left and asylum countries on the right it could have been a bit easier to interpret, but I could not find a layout that would fulfill this.\nI also would have liked it if the original data set included the countries’ population including and excluding the total migration influx. This would be helpful because I feel that it is important to consider per capita when looking at population or migration data. For example, some countries may look like they do not accept a lot of refugees but the reality is that they are already small countries to begin with. Meaning that in reality they are accepting a lot of refugees relative to their population even though it may not seem that way, this can also work vice versa for large countries.\n\n\nConnection to class ideas\nThe main thing I have worked with in this post that connects to what we learned in class is the network graphs. I used a lot of the code foundation from what we did in class working on these. Additionally, I worked a bit with the idea of ethics in data visualization as refugee migration involves a human rights issue being taken into consideration. I feel that when it comes to refugees and interpreting a lot of these conflicts, it can be easy for bias to get involved in trying to draw conclusions about the data."
  },
  {
    "objectID": "posts/blog-post-2/index.html",
    "href": "posts/blog-post-2/index.html",
    "title": "Blog Post 2",
    "section": "",
    "text": "Introduction\nHollywood films are infamous for their ridiculous casting age gaps in films, which can be amusing at times. I think it could be interesting to see what kind of factors from the films could predict how big the age gap could be.\nOn the TidyTuesday GitHub repo I was able to find a data set called “age_gaps” which contains data about different couples from different Hollywood films. The data set includes, variables, such as the film name, the release year, and the name of both the characters and their actors, along with several other variables. The most important is the “age_difference” variable which provides the age difference of the characters in the film couple.\n\n\nQuestion of interest\nWhat factors in the data set can best predict the age gap of different film couples and how well can it really be predicted?\n\n\nData set links\nage_gaps: https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-02-14/age_gaps.csv\n\n\nRead in the data and packages\n\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(here)\nlibrary(modelr)\nlibrary(broom)\nage_gaps &lt;- read_csv(here(\"data/age_gaps.csv\"))\n\n\n\nTidying and wrangling the data\nFor this I did not do a ton of tidying but, I wanted to mutate the character_1_gender so it made more sense. On the website, it is explained that character 1 is the older character, I thought that for the purpose of my model it would make more sense to alter the variable so that it is more clear what the variable is. Thus, I changed the variable of character_1_gender to gendergap, which shows the gender of the older character in the age gap.\n\ntidyagegaps &lt;- age_gaps |&gt; mutate(gendergap = if_else(character_1_gender == \"woman\", true = \"older woman\", false = \"older man\")) |&gt; relocate(gendergap) \ntidyagegaps\n\n# A tibble: 1,155 × 14\n   gendergap   movie_name     release_year director age_difference couple_number\n   &lt;chr&gt;       &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n 1 older woman Harold and Ma…         1971 Hal Ash…             52             1\n 2 older man   Venus                  2006 Roger M…             50             1\n 3 older man   The Quiet Ame…         2002 Phillip…             49             1\n 4 older man   The Big Lebow…         1998 Joel Co…             45             1\n 5 older man   Beginners              2010 Mike Mi…             43             1\n 6 older man   Poison Ivy             1992 Katt Sh…             42             1\n 7 older man   Whatever Works         2009 Woody A…             40             1\n 8 older man   Entrapment             1999 Jon Ami…             39             1\n 9 older man   Husbands and …         1992 Woody A…             38             1\n10 older man   Magnolia               1999 Paul Th…             38             1\n# ℹ 1,145 more rows\n# ℹ 8 more variables: actor_1_name &lt;chr&gt;, actor_2_name &lt;chr&gt;,\n#   character_1_gender &lt;chr&gt;, character_2_gender &lt;chr&gt;,\n#   actor_1_birthdate &lt;date&gt;, actor_2_birthdate &lt;date&gt;, actor_1_age &lt;dbl&gt;,\n#   actor_2_age &lt;dbl&gt;\n\n\n\n\nInitial Visualizations\nTo get a preliminary examination of the data, I made a few minor graphs just to get a feel for the variables I will use in my model.\n\nggplot(data = tidyagegaps, aes(x = actor_1_age, y = age_difference)) + geom_point() + theme_minimal() + labs(y = \"Age Difference\", x = \"Older Actor Age\")\n\n\n\n\nThe above scatter plot shows how the age difference increases with the age of the older actor which makes sense as a large age gap is much more common with a much older actor.\n\nggplot(data = tidyagegaps, aes(x = release_year, y = age_difference)) + geom_line() + theme_minimal() + labs(y = \"Age Difference\", x = \"Release Year\")\n\n\n\n\nThis line plot above shows how age difference changes over the course of time, using release_year as the time variable. The graph did not come out very well as the age gap jumps around a lot, making the line plot so “zig zaggy”. With this plot it is challenging to identify a clear trend.\n\nggplot(data = tidyagegaps, aes(x = gendergap, y = age_difference)) + geom_boxplot() + theme_minimal() + labs(y = \"Age Difference\", x = \"Older Character Gender\")\n\n\n\n\nLastly, the above box plot is perhaps the most interesting and most useful of these three variable plots. The plot is showing the difference in the age gap when the man is older vs. when the woman is. Unsurprisingly to me, there is a much bigger range in age differences when the man is older vs. the woman. This is a pattern I had noticed a lot especially, in older movies. It seems to be much more common to cast a much younger woman with an older man, as opposed to if the woman is older, it is more common to cast a man closer or woman closer to her own age.\n\n\nModelling\nTo make my final visualization of the data I created a linear model predicting age_difference based on release_year, actor_1_age (the age of the older actor), the interaction term between release_year and actor_1_age, and lastly, the gendergap variable\n\nmodtidy &lt;- lm(age_difference ~ release_year + actor_1_age + actor_1_age:release_year + gendergap,\n               data = tidyagegaps)\nmodtidy |&gt; tidy()\n\n# A tibble: 5 × 5\n  term                       estimate std.error statistic  p.value\n  &lt;chr&gt;                         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)              -366.       93.2         -3.93 9.04e- 5\n2 release_year                0.177     0.0466       3.81 1.49e- 4\n3 actor_1_age                13.2       2.13         6.20 8.03e-10\n4 gendergapolder woman       -2.82      0.439       -6.42 1.93e-10\n5 release_year:actor_1_age   -0.00632   0.00106     -5.94 3.82e- 9\n\n\nThe model shows that the y-intercept of the model is negative, with the release_year and actor_1_age coefficients both being positive, but the coefficients of both their interaction term and the gender gap with an older woman are both negative.\nNext, I created a grid of the data, to create an augmented dataset which calculated the confidence intervals for the model.\n\ngridmodtidy &lt;- tidyagegaps |&gt;\n  data_grid(\n    actor_1_age = seq_range(actor_1_age, n = 6),\n    gendergap = c(\"older man\", \"older woman\"),\n    release_year = seq_range(release_year, n = 4)\n  ) \ngridmodtidy\n\n# A tibble: 48 × 3\n   actor_1_age gendergap   release_year\n         &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1        18   older man           1935\n 2        18   older man           1964\n 3        18   older man           1993\n 4        18   older man           2022\n 5        18   older woman         1935\n 6        18   older woman         1964\n 7        18   older woman         1993\n 8        18   older woman         2022\n 9        30.6 older man           1935\n10        30.6 older man           1964\n# ℹ 38 more rows\n\naug_tidy &lt;- augment(modtidy, newdata = gridmodtidy,\n                     interval = \"confidence\")\naug_tidy\n\n# A tibble: 48 × 6\n   actor_1_age gendergap   release_year .fitted .lower .upper\n         &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1        18   older man           1935  -5.85   -9.62  -2.08\n 2        18   older man           1964  -4.01   -6.25  -1.77\n 3        18   older man           1993  -2.17   -3.13  -1.21\n 4        18   older man           2022  -0.327  -1.74   1.08\n 5        18   older woman         1935  -8.67  -12.5   -4.84\n 6        18   older woman         1964  -6.83   -9.15  -4.52\n 7        18   older woman         1993  -4.99   -6.07  -3.91\n 8        18   older woman         2022  -3.15   -4.61  -1.69\n 9        30.6 older man           1935   6.40    4.21   8.59\n10        30.6 older man           1964   5.93    4.64   7.23\n# ℹ 38 more rows\n\n\n\n\nPlot of the model\nFinally, I made a faceted line and scatter plot of the model, using the original data for the scatter plot. But the, augmented data for both the line plot, and the confidence intervals. I faceted the graph by gendergap and treated release_year as a factor, which split it into 4 of its most common values for each line on the graph, since I set n to 4 in the grid, I could not use every year value as it would be too many and overwhelm the graph.\n\nggplot(data = age_gaps, aes(x = actor_1_age, y = age_difference)) + geom_point(alpha = 0.3) + geom_ribbon(data = aug_tidy, aes(y = .fitted, ymin = .lower, ymax = .upper, fill = as.factor(release_year)), alpha = 0.4) + geom_line(data = aug_tidy, aes(x = actor_1_age, y = .fitted, colour = as.factor(release_year)), linewidth = 2) + facet_wrap(~gendergap) + theme_minimal() + labs(colour = \"Release Year\", y = \"Age Difference\", x = \"Older Actor Age\", fill = \"Release Year Confidence Interval\")\n\n\n\n\nThe resulting plot is a bit odd, due to such a negative y-intercept, the entire plot is shifted below zero, which is the main thing altering the graphs appearance so severely. However, there is a clear interaction point in the lines around 30 years for the older actor age. Additionally, it is quite obvious that the age difference increases with age of the older actor. The lines show that, the older movies had much bigger age gaps than the more recent ones which is unsurprising as big age gaps were much more common in older films. Interestingly, the confidence intervals seem to be bigger for the older movies than the newer ones.The lines in both individual plots are parallel which is to be expected. However, the lines in the graph with the older woman are slightly lower than in the one with the older man, this relates to the trend I observed in the side by side box plots. In summary, the conclusion, I can come to using this graph is that the age difference in between actors in films increases with the age of the older actor, and that difference tends to be greater when the man is older. Lastly, the age difference have gradually gone down over time in more recent films, meaning hopefully these big age gaps are a thing of the past.\n\n\nConclusion and wrap-up\nIn conclusion, I feel that this model was a successful method answering my question of interest, of what best predicts the age gap in between actors in films. I found it an interesting and challenging method to answer the question. That being said, there are a couple of important things to note with the data, that most likely impacted my model and had I more time I would have liked to address. One, is that some films are listed multiple times in the data set as they had multiple couples, for me this did not matter a ton because I was just interested in the age gaps not necessarily the films, but if I had more time it would have been better to address this issue. Secondly and more majorly, there is the fact that some couples are not just a man and a woman but could also be two men or two women, I do not feel that my model did a great job accounting for this and I would have liked to do that better. Lastly, I just simply would have liked to tidy the data a bit more, it was a bit challenging to work with but certainly interesting.\n\n\nConnection to class ideas\nEverything, in this post certainly related to something we learned in class, but I feel that the overarching idea of the modelling was the most obvious connection. I based my model very heavily off of the evals example we had done in section 9 in class, using that coding structure for inspiration, and using that to help guide me in interpreting the data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Abigail’s Data Visualization Blog",
    "section": "",
    "text": "Final Project\n\n\nBlog Post 4\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nAbigail Smith\n\n\n\n\n\n\n  \n\n\n\n\nPatterns in Global Refugee Migration\n\n\nBlog Post 3\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nAbigail Smith\n\n\n\n\n\n\n  \n\n\n\n\nHollywood Age Gaps\n\n\nBlog Post 2\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nAbigail Smith\n\n\n\n\n\n\n  \n\n\n\n\nGroundhog Day Predictions\n\n\nBlog Post 1\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nAbigail Smith\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my data visualization blog. I have a series of posts working with different data sets I have found on the Tidy Tuesday repository on GitHub. My personal GitHub account is accessible through the above GitHub icon. In each of my posts, I work with different types of visualizations that I have learned about in my Data334 (Data Visualization) course."
  },
  {
    "objectID": "posts/blog_post_4/index.html",
    "href": "posts/blog_post_4/index.html",
    "title": "Final Project",
    "section": "",
    "text": "Introduction\nFor the project I am interested in investigating the impact of the sticky stuff scandal on baseball as a whole. I will do so by creating a Shiny App using the StatCast data as well as some attendance and TV viewership data I found online. In looking at the attendance data I am curious to see if there is a noticeable pattern between the trend in attendance and the trends in spin rate. More specifically, I would like to see if attendance increased after the MLB cracked down on sticky substances. My theory is that I will see an increase in attendance following 2021, as generally speaking attendance is higher when the amount of home runs is higher. Take for example, the steroid era in the early 2000s, home runs were at an all time high because so many players were taking steroids, and attendance was extremely high as well. This leads me to believe that something like the sticky stuff scandal would have the opposite effect and harm attendance. I also think a brief glance at some world series tv viewership data I found will be relevant to this same idea. Although in this situation the sample size is smaller, I still think that a trend in world series viewership reflects on the viewership and popularity of baseball in that season as a whole. I would also like to see just how evident the impact of spin rate on batting statistics such as home runs, batting average, and slugging average are. Similarly, I want to see how spin rate impacted other pitching statistics, such as strike out percentage and the number of players hit by pitch. On a simple note, I would like to take a glance at pitching how spin rate has changed over time just to see how evident the direct influence of the sticky stuff scandal is.\n\n\nPackages\n\n\nData\n\n# Reading in the pitching data set\npitching_df &lt;- read_csv(here(\"data/pitchingstats.csv\"))\n\nRows: 768 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): last_name, first_name\ndbl (17): player_id, year, pa, home_run, k_percent, bb_percent, p_hit_by_pit...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Reading in each individual attendance data set\nattendance_2018 &lt;- read_csv(here(\"data/2018attendance.csv\"))\n\nRows: 30 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Tm, Est. Payroll, Succ%, Managers\ndbl  (9): BatAge, PAge, BPF, PPF, #HOF, #A-S, #a-tA-S, Chall, Succ\nnum  (2): Attendance, Attend/G\ntime (1): Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nattendance_2019 &lt;- read_csv(here(\"data/2019attendance.csv\"))\n\nRows: 30 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Tm, Est. Payroll, Succ%, Managers\ndbl  (9): BatAge, PAge, BPF, PPF, #HOF, #A-S, #a-tA-S, Chall, Succ\nnum  (2): Attendance, Attend/G\ntime (1): Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nattendance_2020 &lt;- read_csv(here(\"data/2020attendance.csv\"))\n\nRows: 30 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Tm, Est. Payroll, Succ%, Managers\ndbl  (9): BatAge, PAge, BPF, PPF, #HOF, #A-S, #a-tA-S, Chall, Succ\nlgl  (2): Attendance, Attend/G\ntime (1): Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nattendance_2021 &lt;- read_csv(here(\"data/2021attendance.csv\"))\n\nRows: 30 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Tm, Est. Payroll, Succ%, Managers\ndbl  (9): BatAge, PAge, BPF, PPF, #HOF, #A-S, #a-tA-S, Chall, Succ\nnum  (2): Attendance, Attend/G\ntime (1): Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nattendance_2022 &lt;- read_csv(here(\"data/2022attendance.csv\"))\n\nRows: 30 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Tm, Est. Payroll, Succ%, Managers\ndbl  (9): BatAge, PAge, BPF, PPF, #HOF, #A-S, #a-tA-S, Chall, Succ\nnum  (2): Attendance, Attend/G\ntime (1): Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nattendance_2023 &lt;- read_csv(here(\"data/2023attendance.csv\"))\n\nRows: 30 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Tm, Est. Payroll, Succ%, Managers\ndbl  (9): BatAge, PAge, BPF, PPF, #HOF, #A-S, #a-tA-S, Chall, Succ\nnum  (2): Attendance, Attend/G\ntime (1): Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nattendance_2024 &lt;- read_csv(here(\"data/2024attendance.csv\"))\n\nRows: 30 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Tm, Est. Payroll, Succ%, Managers\ndbl  (9): BatAge, PAge, BPF, PPF, #HOF, #A-S, #a-tA-S, Chall, Succ\nnum  (2): Attendance, Attend/G\ntime (1): Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Reading in the world series data set\nworldseriesviewing &lt;- read_csv(here(\"data/world-series-ratings.csv\"))\n\nRows: 54 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): network, winning_team, losing_team\ndbl (11): year, average_audience, game_1_audience, game_2_audience, game_3_a...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# Creating a year variable in each of the attendance data sets\nattendance_2018$year &lt;- 2018\nattendance_2019$year &lt;- 2019\nattendance_2020$year &lt;- 2020\nattendance_2021$year &lt;- 2021\nattendance_2022$year &lt;- 2022\nattendance_2023$year &lt;- 2023\nattendance_2024$year &lt;- 2024\n\n\n# Create 3 joined data sets of the attendance data sets\nattendance2018_2019 &lt;- full_join(attendance_2018, attendance_2019)\n\nJoining with `by = join_by(Tm, Attendance, `Attend/G`, BatAge, PAge, BPF, PPF,\n`#HOF`, `#A-S`, `#a-tA-S`, `Est. Payroll`, Time, Chall, Succ, `Succ%`,\nManagers, year)`\n\nattendance2020_2021 &lt;- full_join(attendance_2020, attendance_2021)\n\nJoining with `by = join_by(Tm, Attendance, `Attend/G`, BatAge, PAge, BPF, PPF,\n`#HOF`, `#A-S`, `#a-tA-S`, `Est. Payroll`, Time, Chall, Succ, `Succ%`,\nManagers, year)`\n\nattendance2022_2023 &lt;- full_join(attendance_2022, attendance_2023)\n\nJoining with `by = join_by(Tm, Attendance, `Attend/G`, BatAge, PAge, BPF, PPF,\n`#HOF`, `#A-S`, `#a-tA-S`, `Est. Payroll`, Time, Chall, Succ, `Succ%`,\nManagers, year)`\n\n# Join those 3 data sets together into 2 datasets\nattendance2018_2021 &lt;- full_join(attendance2018_2019, attendance2020_2021)\n\nJoining with `by = join_by(Tm, Attendance, `Attend/G`, BatAge, PAge, BPF, PPF,\n`#HOF`, `#A-S`, `#a-tA-S`, `Est. Payroll`, Time, Chall, Succ, `Succ%`,\nManagers, year)`\n\nattendance2022_2024 &lt;- full_join(attendance2022_2023, attendance_2024)\n\nJoining with `by = join_by(Tm, Attendance, `Attend/G`, BatAge, PAge, BPF, PPF,\n`#HOF`, `#A-S`, `#a-tA-S`, `Est. Payroll`, Time, Chall, Succ, `Succ%`,\nManagers, year)`\n\n# Join the last two big data sets in one big complete attendance data set\nattendancefull &lt;- full_join(attendance2018_2021, attendance2022_2024)\n\nJoining with `by = join_by(Tm, Attendance, `Attend/G`, BatAge, PAge, BPF, PPF,\n`#HOF`, `#A-S`, `#a-tA-S`, `Est. Payroll`, Time, Chall, Succ, `Succ%`,\nManagers, year)`\n\n\n\n# Save the attendance data set as a CSV so that I do not lose track of it\nwrite_csv(attendancefull, \"attendancefull.csv\")\n\n\n\nCleaning and tidying the pitching data\n\npitching_df &lt;- pitching_df |&gt; rename(pitcher = `last_name, first_name`)\n\n\npitching_cleaner &lt;- pitching_df |&gt; pivot_longer(c(\"sl_avg_spin\", \"ch_avg_spin\", \"cu_avg_spin\", \"si_avg_spin\"), names_to = \"pitch_type\", values_to =  \"spin_rate\")\n\npitching_cleaner &lt;- pitching_cleaner |&gt; mutate(pitch_type = str_remove(pitch_type, \"_avg_spin\")) |&gt;  mutate(pitch_type = case_when(pitch_type == \"sl\" ~ \"slider\", pitch_type == \"ch\" ~ \"changeup\", pitch_type == \"cu\" ~ \"curveball\", pitch_type == \"si\" ~ \"sinker\"))  \n\npitching_cleaner &lt;- pitching_cleaner |&gt; select(!c(sl_avg_speed, ch_avg_speed, si_avg_speed, cu_avg_speed, pa, player_id)) \n\n\n\nCleaning up attendance and tv viewership dataframes\nI do not need all of these variables in the data frame so I am going to get rid of some of them\n\nattendance_smaller &lt;- attendancefull |&gt; select(Tm, Attendance, `Attend/G`, year)\n\nSame deal here with the tv viewership\n\ntv_views_smaller &lt;- worldseriesviewing |&gt; select(c(year, average_audience)) |&gt; filter(year &gt;= 2018 & year &lt;= 2023)\n\n\n\nStatic visualizations for the shiny app\n\nattendance_smaller_reorder &lt;- attendance_smaller |&gt; mutate(Tm = fct_reorder(Tm, `Attend/G`))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `Tm = fct_reorder(Tm, `Attend/G`)`.\nCaused by warning:\n! `fct_reorder()` removing 33 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\n\nggplot(data = attendance_smaller_reorder, aes(x = Tm, y = `Attend/G`)) + geom_col() + coord_flip() + theme_minimal()\n\nWarning: Removed 33 rows containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\nggplot(data = attendance_smaller, aes(x = year, y = `Attend/G`, colour = Tm)) + geom_line()\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\nggplot(data = tv_views_smaller, aes(x = year, y = average_audience)) + geom_col()\n\n\n\n\n\nggplot(data = pitching_cleaner, aes(x= spin_rate, y = home_run, colour = pitch_type)) + geom_point()\n\nWarning: Removed 579 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\nggplot(data = pitching_cleaner, aes(x = spin_rate)) + geom_histogram() + facet_wrap(~year) + theme_minimal() + labs(title = \"Histogram of spin rate facted by year\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 579 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\nShiny app\n\nlibrary(shiny)\nyear_choices_pitching &lt;- pitching_cleaner |&gt; distinct(year) |&gt; pull(year)\ny_var_choices &lt;- names(pitching_cleaner)[c(3:8)]\npitch_choice &lt;- pitching_cleaner |&gt; distinct(pitch_type) |&gt; pull(pitch_type)\nx_var &lt;- names(attendance_smaller)[c(2, 3)]\nyear_choices_attendance &lt;- attendance_smaller|&gt; distinct(year) |&gt; pull(year)\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      radioButtons(\"year_sel\", label = \"Choose a year for pitching:\", choices = year_choices_pitching),\n      selectizeInput(\"pitch_sel\", label = \"Choose pitch type:\",\n                     choices = pitching_cleaner$pitch_type, selected = \"slider\",\n                     multiple = TRUE),\n      selectInput(\"y_var_sel\", label = \"Choose a response variable for pitching:\", choices = y_var_choices),\n      radioButtons(\"x_sel\", label = \"Choose an attendance variable:\", choices = x_var),\n      radioButtons(\"year_sel_attendance\", label = \"Choose a year for the attendance:\", choices = year_choices_pitching),\n      selectizeInput(\"tm_sel\", label = \"Choose a team:\", choices = attendance_smaller$Tm, selected = \"Arizona Diamondbacks\", multiple = TRUE)\n    ),\n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"Pitching\", plotOutput(\"scatterplot\"), plotOutput(\"spinhist\"),  dataTableOutput(\"pitching_raw_data\")),\n        tabPanel(\"Attendance\", plotOutput(\"attendance_line\"), plotOutput(\"barplot\"), dataTableOutput(\"attendance_raw_data\")),\n        tabPanel(\"TV Viewership\", plotOutput(\"audience_bar\"), dataTableOutput(\"audience_raw_data\"))\n      )\n    )\n  )\n  \n)\n\nWarning: The select input \"pitch_sel\" contains a large number of options;\nconsider using server-side selectize for massively improved performance. See\nthe Details section of the ?selectizeInput help topic.\n\nserver &lt;- function(input, output, session) {\n pitching_reactive &lt;- reactive({\n   pitching1 &lt;- pitching_cleaner |&gt; filter(year == input$year_sel) |&gt; filter(pitch_type %in% input$pitch_sel)\n})\n \noutput$scatterplot &lt;- renderPlot({ggplot(pitching_reactive(), aes(x= spin_rate, y = .data[[input$y_var_sel]], colour = pitch_type)) + geom_point() + theme_minimal() + labs(title = glue::glue(\"Scatterplot of spin rate against\",  input$y_var_sel, \" in \", input$year_sel))})\n\n\noutput$spinhist &lt;- renderPlot({ggplot(data = pitching_cleaner, aes(x = spin_rate)) + geom_histogram() + facet_wrap(~year) + theme_minimal() + labs(title = \"Histogram of spin rate facted by year\")})\n\n  attendance_reactive &lt;- reactive({\n    attendance1 &lt;- attendance_smaller |&gt; filter(year == input$year_sel_attendance) |&gt;  mutate(Tm = fct_reorder(Tm, .data[[input$x_sel]]))\n  })\n  \n   attendancereactive &lt;- reactive({\n   team1 &lt;- attendance_smaller |&gt; filter(Tm %in% input$tm_sel)})\noutput$barplot &lt;- renderPlot({ggplot(attendance_reactive(), aes(x = Tm, y = .data[[input$x_sel]])) + geom_col() + coord_flip() + theme_minimal() + labs(title = glue::glue(\"Barplot of \", input$x_sel, \" in \", input$year_sel_attendance)) })\n\noutput$attendance_line &lt;- renderPlot({ggplot(attendancereactive(), aes(x = year, y = `Attend/G`, colour = Tm)) + geom_line() + theme_minimal() + labs(title = \"Average Attendance per Game Over Time \") })\n\noutput$audience_bar &lt;- renderPlot({ggplot(data = tv_views_smaller, aes(x = year, y = average_audience)) + geom_col() + theme_minimal() + labs(title = \"Average TV Audience per World Series Game In Each Year \") })\n\n\noutput$attendance_raw_data &lt;- renderDataTable({attendance_smaller})\n\noutput$audience_raw_data &lt;- renderDataTable({tv_views_smaller})\n\noutput$pitching_raw_data &lt;- renderDataTable({pitching_cleaner})\n}\n\nshinyApp(ui, server)\n\nShiny applications not supported in static R Markdown documents\n\n\n\n\nFindings\nWhen playing around with the app, I found that there is some pretty clear difference in the hitting statistics before 2021 as opposed to after. Things like strike out percentage were higher prior to 2021 as opposed to things like opposing home runs which went up after 2021. The impact of the sticky substances is most evident in the faceted histogram of spin rate, it is quite clear that the spin rate was higher previous to 2021. In the attendance graphs as well, there is a considerable increase in both the average and total attendance following 2021. This change is somewhat less evident in the TV viewership bar plot however. What is most evident in all graphs is the 2020 season, this is especially interesting in the line graph which shows a gap during the 2020 season which is because attendance was not allowed during the COVID-19 pandemic. Additionally, the 2020 season was shortened so all statistics for that season will be offset. Another season in the data set that will be strange is the 2024 season since that one had only just begun at the time of me making the app."
  }
]